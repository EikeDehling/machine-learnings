{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Workshop : Text Classification\n",
    "\n",
    "In this workshop we'll learn about a NLP (Natural Language Processing) technique called Text Classification. This means to which category a piece of text belongs. An example application of this is sentiment analysis, detecting positive or negative texts.\n",
    "\n",
    "We will use a dataset from crowdflower about hate speech - the use case is detecting offensive language on social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We start by loading the dataset, for this we use [Pandas](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/twitter-hate-speech.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick dataset overview\n",
    "\n",
    "Now that data is loaded we'll habe a quick look at what's available, what information do we have?\n",
    "\n",
    "I'll explain the columns you see in the dataset sample output:\n",
    "\n",
    "- **count** : Number of human annotations for this sample\n",
    "- **hate_speech** : Times annotated as containing hate speech\n",
    "- **offensive_language** : Times annotated as containing offensive language\n",
    "- **neither** : Times annotated as not containing hatefull of offensive language (normal, respectfull language)\n",
    "- **class** : Human annotated category, max votes determines category (0=Hate, 1=Offensive, 2=Neither)\n",
    "- **tweet** : The tweets text\n",
    "\n",
    "From these available columns we'll use **class** as the value we try to predict and **tweet** as input to determine the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp;amp; as a man you should always take the trash ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &amp;#57361;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                                                                                                                         tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash ou...  \n",
       "1                                                        !!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!  \n",
       "2                     !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit  \n",
       "3                                                                               !!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny  \n",
       "4    !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 140)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how much data there is in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Plot the distribution of class in the dataset\n",
    "\n",
    "An usefull insight can be to have a look at the distriution of categories in the dataset. Plotting can be done using the [Seaborn library](https://seaborn.pydata.org/).\n",
    "\n",
    "**Your assignment is to plot the distrubution (count) of the categories.**\n",
    "\n",
    "After plotting you will notice that the categories are not evenly distributed, category one has many more samples than the others. We'll get back to this later!\n",
    "\n",
    "*Hint: Look at the countplot function in seaborn documentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.countplot(...)  # This line needs to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text preprocessing\n",
    "\n",
    "In the data above you can see that the text of the tweets contains lot's of slang words, social media specific abbreviations and symbols mixed in. Also there is usernames and hashtags in the text, which we might not want a model to take into consideration for classifying (We'd like the model to learn hatefull/offensive keywords, rather than remember which users use bad language)\n",
    "\n",
    "For this we'll now look at text preprocessing to clean up the data.\n",
    "\n",
    "Two libraries we will use here are [sklean](http://scikit-learn.org) and [gensim](https://radimrehurek.com/gensim/).\n",
    "\n",
    "You can read more here:\n",
    "- [Gensim text preprocessing](https://radimrehurek.com/gensim/parsing/preprocessing.html)\n",
    "- [Sklearn text feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "First i will show what the default sklearn and gensim preprocessing functions do, then we'll have a deeper look and customize our own preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace splitting\n",
    "\n",
    "First let's see how the output looks when we simply split on whitespace. What you will see is that the tweets are simpy split up into separate words, all noise such as symbols is retained of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [!!!, RT, @mayasolovely:, As, a, woman, you, shouldn't, complain, about, cleaning, up, your, house., &amp;, as, a, man, you, should, alw...\n",
       "1                                         [!!!!!, RT, @mleew17:, boy, dats, cold...tyga, dwn, bad, for, cuffin, dat, hoe, in, the, 1st, place!!]\n",
       "2    [!!!!!!!, RT, @UrKindOfBrand, Dawg!!!!, RT, @80sbaby4life:, You, ever, fuck, a, bitch, and, she, start, to, cry?, You, be, confused, as,...\n",
       "3                                                                       [!!!!!!!!!, RT, @C_G_Anderson:, @viva_based, she, look, like, a, tranny]\n",
       "4    [!!!!!!!!!!!!!, RT, @ShenikaRoberts:, The, shit, you, hear, about, me, might, be, true, or, it, might, be, faker, than, the, bitch, who,...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].head(n=5).apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn default tokenizer\n",
    "\n",
    "Next up is the sklean tokenizer. This already does some text cleaning, such as removing symbols and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [rt, mayasolovely, woman, shouldn, complain, cleaning, house, amp, man, trash]\n",
       "1       [rt, mleew17, boy, dats, cold, tyga, dwn, bad, cuffin, dat, hoe, 1st, place]\n",
       "2    [rt, urkindofbrand, dawg, rt, 80sbaby4life, fuck, bitch, start, confused, shit]\n",
       "3                                 [rt, c_g_anderson, viva_based, look, like, tranny]\n",
       "4              [rt, shenikaroberts, shit, hear, true, faker, bitch, told, ya, 57361]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sklearn_default_preprocessor = CountVectorizer(strip_accents='unicode', stop_words='english').build_analyzer()\n",
    "\n",
    "df['tweet'].head(n=5).apply(sklearn_default_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim preprocessor\n",
    "\n",
    "Finally we will have a look at the gensim preprocessor. This does even more cleaning of the text, for example removing short tokens, numbers and it does stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mayasolov, woman, shouldn, complain, clean, hous, amp, man, trash]\n",
       "1       [mleew, boi, dat, cold, tyga, dwn, bad, cuffin, dat, hoe, place]\n",
       "2      [urkindofbrand, dawg, sbabylif, fuck, bitch, start, confus, shit]\n",
       "3                             [anderson, viva, base, look, like, tranni]\n",
       "4                  [shenikarobert, shit, hear, true, faker, bitch, told]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "df['tweet'].head(n=5).apply(preprocess_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Customized preprocessor\n",
    "\n",
    "We have shown a few different approaches for preprocessing text, now we'll create a customized preprocessor that does some extra social media specific data cleaning.\n",
    "\n",
    "For example:\n",
    "- Remove usernames\n",
    "- Remove 'hash' from hashtags\n",
    "- No stemming\n",
    "\n",
    "**Take a look at the code below and complete the preprocessing functions**\n",
    "\n",
    "*Hint: Look at how to lowercase strings and how to use regular expressions in python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [mayasolovely, woman, shouldn, complain, cleaning, house, amp, man, trash]\n",
       "1                 [mleew, boy, dats, cold, tyga, dwn, bad, cuffin, dat, hoe, place]\n",
       "2    [UrKindOfBrand, Dawg, sbabylife, You, fuck, bitch, start, You, confused, shit]\n",
       "3                                       [Anderson, viva, based, look, like, tranny]\n",
       "4                       [ShenikaRoberts, The, shit, hear, true, faker, bitch, told]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, \\\n",
    "    remove_stopwords\n",
    "\n",
    "def drop_short(tweet):\n",
    "    # This function is included as an example, it removes short tokens\n",
    "    return ' '.join(x for x in tweet.split() if len(x) >= 3)\n",
    "\n",
    "def to_lowercase(tweet):\n",
    "    return tweet  # TODO Complete this function\n",
    "\n",
    "def drop_usernames(tweet):\n",
    "    return tweet  # TODO Complete this function\n",
    "    \n",
    "my_filters = [ to_lowercase, drop_usernames, strip_multiple_whitespaces, strip_punctuation, strip_numeric,\n",
    "               remove_stopwords, drop_short ]\n",
    "\n",
    "df['tweet'].head(n=5).apply(lambda x: preprocess_string(x, my_filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature creation\n",
    "\n",
    "In the above section we have preprocessed the text of the tweets and removed noisy or undesireable words. Also the tweets have been split into separate words, this is called tokenization. With this preparation done, we are now ready to transform the data into a format suitable for a machine learning model.\n",
    "\n",
    "Machine learning models generally require numerical input, they don't work on text or words directly. Also machine learning models usually require a fixed amount of input columns or features. So in this section we will transform the variable-length tokenized tweets into a fixed set of features.\n",
    "\n",
    "One method of of transforming variable-length texts to a fixed set of numerical features is using each unique word as a feature, and using the count of that word in the text as the value. This is called bad-of-words, below is an image illustrating this, there is some example sentences and the table shows them transformed into bag-of-words features:\n",
    "\n",
    "1. I love machine learning\n",
    "2. I hate learning boring things\n",
    "3. Machine learning is a passion\n",
    "\n",
    "\n",
    "| Sentence   | I | Love | Machine | Learning | Hate | Boring | Things | Is | A | Passion |\n",
    "| ---------- |:-:|:----:|:-------:|:--------:|:----:|:------:|:------:|:--:|:-:|:-------:|\n",
    "| Sentence 1 | 1 |    1 |       1 |        1 |    0 |      0 |      0 |  0 | 0 |       0 |\n",
    "| Sentence 2 | 1 |    0 |       1 |        0 |    1 |      1 |      1 |  0 | 0 |       0 |\n",
    "| Sentence 3 | 0 |    0 |       1 |        1 |    0 |      0 |      0 |  1 | 1 |       1 | \n",
    "\n",
    "We are going to do the this now for the tweets using a utility from sklearn, a CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(df['tweet'].values)\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at what the output is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24783x35573 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 210603 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 35573)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this transformation has resulted in a matrix of 35573 feature columns, that's probably a few to many. Reason for this is, there is many words appearing once or twice. A machine learning algorithm can't learn much from words that appear so infrequently, or in any case the patterns that it might learn won't apply to many new tweets. So we can safely filter out a lot here. The easiest way is to filter by frequency, we simply drop tokens that appear only in a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, strip_accents='unicode', stop_words='english')\n",
    "\n",
    "X_filtered = vectorizer.fit_transform(df['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 4693)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Filtering tokens\n",
    "\n",
    "In this exercise we will experiment with filtering tokens by frequency to remove low-frequency tokens, since they would likely not be very usefull anyway.\n",
    "\n",
    "**Experiment with the code below to obtain a feature-matrix of around 500 to 1000 features.**\n",
    "\n",
    "*Hint: Have a look at the min_df parameter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=..., strip_accents='unicode', stop_words='english')\n",
    "\n",
    "X_filtered_more = vectorizer.fit_transform(df['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered_more.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from preprocessed data\n",
    "\n",
    "In the examples above we haven't yet used our preprocessing logic, we had just split up words as default. Let's do this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, strip_accents='unicode', analyzer='word',\n",
    "                             tokenizer=preprocess_string, stop_words='english')\n",
    "\n",
    "X_preprocessed = vectorizer.fit_transform(df['tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 2164)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: Plug in our custom preprocessor\n",
    "\n",
    "In this exercise you will plug the custom preprocessor we created earlier in to the vectorizer.\n",
    "\n",
    "**Adjust the code below to use the custom preprocessor logic**\n",
    "\n",
    "*Hint: Use a lambda function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, strip_accents='unicode', analyzer='word',\n",
    "                             tokenizer=..., stop_words='english')\n",
    "\n",
    "X_custom = vectorizer.fit_transform(df['tweet'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing the preprocessing / feature approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All score: 0.87 (+/- 0.00)\n",
      "Filtered score: 0.89 (+/- 0.00)\n",
      "Filtered Moe score: 0.87 (+/- 0.00)\n",
      "Preprocessed score: 0.89 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "print(\"%s score: %0.2f (+/- %0.2f)\" % ('All', scores.mean(), scores.std()))\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X_filtered, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "print(\"%s score: %0.2f (+/- %0.2f)\" % ('Filtered', scores.mean(), scores.std()))\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X_filtered_more, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "print(\"%s score: %0.2f (+/- %0.2f)\" % ('Filtered more', scores.mean(), scores.std()))\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X_preprocessed, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "print(\"%s score: %0.2f (+/- %0.2f)\" % ('Preprocessed', scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression score: 0.90 (+/- 0.00)\n",
      "Model MultinomialNB score: 0.89 (+/- 0.00)\n",
      "Model DecisionTreeClassifier score: 0.88 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(multi_class='multinomial', solver='newton-cg'),\n",
    "    MultinomialNB(),\n",
    "    DecisionTreeClassifier(min_samples_split=50)\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X_count, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "    print(\"Model %s score: %0.2f (+/- %0.2f)\" % (model.__class__.__name__, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticRegression score: 0.83 (+/- 0.00)\n",
      "Model DecisionTreeClassifier score: 0.83 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(multi_class='multinomial', solver='newton-cg', class_weight='balanced'),\n",
    "    DecisionTreeClassifier(min_samples_split=50, class_weight='balanced')\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X_count, y, cv=ShuffleSplit(n_splits=10, test_size=0.2))\n",
    "    print(\"Model %s score: %0.2f (+/- %0.2f)\" % (model.__class__.__name__, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Hate       0.46      0.23      0.31       294\n",
      "  Offensive       0.91      0.96      0.93      3850\n",
      "    Neither       0.84      0.78      0.81       813\n",
      "\n",
      "avg / total       0.87      0.89      0.88      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2)\n",
    "\n",
    "lr = MultinomialNB()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, lr.predict(X_test), target_names=('Hate', 'Offensive', 'Neither')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
