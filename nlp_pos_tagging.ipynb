{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech tagging\n",
    "\n",
    "Part of speech tagging means assigning the correct part of speech labels to words in a sentence. In itself might appear to be a toy problem, but the part of speech label can be actually important for further models, such as entity extraction or sentiment.\n",
    "\n",
    "It's also an interesting problem because you're tagging all the words in a sentence. So it's more than just classifying or rules. So the commonly also specialized machine learning models are used which take this sequence structure into account.\n",
    "\n",
    "In this notebook a brief look at some off-the-shelf approaches and we also attempt to roll our own model, just to further our understanding of the topic.\n",
    "\n",
    "\n",
    "## Data overview\n",
    "\n",
    "Let's start by loading the data and seeing what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Elsevier', 'NNP'),\n",
       " ('N.V.', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Dutch', 'NNP'),\n",
       " ('publishing', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data consists of a bunch of sentences, with for each sentence the words and the associated Part of Speech tags.\n",
    "\n",
    "## First: Ngram Tagger\n",
    "\n",
    "The first model we'll have a go at is a UnigramTagger, this is a naive model that just assigns the most frequent PoS tag observed for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score =  0.8875074955026984\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.sequential import DefaultTagger, UnigramTagger, BigramTagger\n",
    "\n",
    "model = UnigramTagger(train_sentences, backoff=DefaultTagger('NN'))\n",
    "\n",
    "print(\"Model score = \", model.evaluate(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One step better is a Bigram Tagger, this takes the previous words Part of Speech tag into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score =  0.896662002798321\n"
     ]
    }
   ],
   "source": [
    "model = BigramTagger(train_sentences, backoff=UnigramTagger(train_sentences, backoff=DefaultTagger('NN')))\n",
    "\n",
    "print(\"Model score = \", model.evaluate(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Tagger\n",
    "\n",
    "The models in the previous section are quite limited, as such their performance leaves one wanting.\n",
    "\n",
    "Another approach that's popular are HMM models, these models explicitly model the relation between sequential observations and seem quite suitabe for the problem.\n",
    "\n",
    "Let's see how this model does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score =  0.907615430741555\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "model = HiddenMarkovModelTagger.train(train_sentences)\n",
    "\n",
    "print(\"Model score = \", model.evaluate(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Tagger\n",
    "\n",
    "The last model we tried was an improvement, but we're still not where we'd like to be. Another option offered by the nltk library is a CRF model. These models can use multiple features and also explicitly model the sequential relation in a sentence.\n",
    "\n",
    "Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score =  0.9491505096941835\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "\n",
    "model = CRFTagger()\n",
    "model.train(train_sentences, 'model.bin')\n",
    "\n",
    "print(\"Model score = \", model.evaluate(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Tagger\n",
    "\n",
    "The last built-in model we'll try out is a perceptron tagger. It's based on a neural network with various features that capture the sequential relation. Supposedly this is the \"gold standard\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score =  0.972856286228263\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "model = PerceptronTagger()\n",
    "model.train(train_sentences)\n",
    "\n",
    "print(\"Model score = \", model.evaluate(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll our own tagger\n",
    "\n",
    "It's certainly interesting to experiment with the built-in taggers NLTK offers, but as curious engineers we won't stop there of course. How does such a model work internally and what features do we need to make good predictions?\n",
    "\n",
    "In the section below we'll attempt to make a tagger using a sklean classifier and an appriopriate list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we do, is define a feature extraction function. This takes as input a sentence and a position. Output is a dictionary of features.  If you look in the NLTK source, a CRF model and their PerceptronTagger use a very similar feature function.\n",
    "\n",
    "So let's hope we are close to their accuracy levels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index].lower(),\n",
    "        #'stem': stemmer.stem(sentence[index]).lower(),\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_title': sentence[index][0].istitle(),\n",
    "        'is_upper': sentence[index].isupper(),\n",
    "        'is_lower': sentence[index].islower(),\n",
    "        'is_digit': sentence[index].isdigit(),\n",
    "        'has_upper_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'prefix-1': sentence[index][0].lower(),\n",
    "        'prefix-2': sentence[index][:2].lower(),\n",
    "        'prefix-3': sentence[index][:3].lower(),\n",
    "        'suffix-1': sentence[index][-1].lower(),\n",
    "        'suffix-2': sentence[index][-2:].lower(),\n",
    "        'suffix-3': sentence[index][-3:].lower(),\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1].lower(),\n",
    "        #'prev_stem': '' if index == 0 else stemmer.stem(sentence[index - 1]).lower(),\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1].lower(),\n",
    "        #'next_stem': '' if index == len(sentence) - 1 else stemmer.stem(sentence[index + 1]).lower(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need to transform the dataset. Sklearn works on a single X and y vector, while NLTK uses lists of sentences. So we add a small function to translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features([w for w, t in tagged], index))\n",
    "            y.append(tagged[index][1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the model. Here i've chosen to use a small neural network, other classifiers also work though. A neural network seemed to work particularly well here, i presume because there are more than two labels and the interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    SelectKBest(k=5000),\n",
    "    MLPClassifier(hidden_layer_sizes=(12,), early_stopping=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did we do?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dehling/Documents/machine-learnings/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9516290225864481\n"
     ]
    }
   ],
   "source": [
    "clf.fit(*transform_to_dataset(train_sentences))\n",
    "\n",
    "print(\"Accuracy:\", clf.score(*transform_to_dataset(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
